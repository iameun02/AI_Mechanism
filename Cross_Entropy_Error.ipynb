{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml-hcS25cMbV"
      },
      "source": [
        "# Cross Entropy Error"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "szaCiyErS8dR"
      },
      "source": [
        "\n",
        "## 분류 모델 학습원리\n",
        "\n",
        "회귀모델과 비교시, 실측값과 예측값의 차이를 가지고 경사하강법을 적용시킨다는 원리는 같지만, <br> \n",
        "회귀식에서는 오차함수식에 MSE 지표를 활용하고, <br>\n",
        "분류식에서는 CEE를 활용한다. (더 잘 맞기 때문) <br> <br>\n",
        "\n",
        "<b>Binary cross entropy error</b> <br>\n",
        "\n",
        "-𝒚 * 𝓵𝓸𝓰(𝒚𝒉𝒂𝒕) - (1-𝒚) * 𝓵𝓸𝓰(1 - 𝒚𝒉𝒂𝒕) <br>\n",
        "\n",
        "𝒚𝒉𝒂𝒕은 시그모이드 함수에 의해 0과 1사이 값 으로만 존재하게 된다. <br><br>\n",
        "<b>𝒚 = 0, 𝐂𝐄𝐄 = - 𝓵𝓸𝓰(1 - 𝒚𝒉𝒂𝒕) </b>\n",
        "  - 𝒚 = 0, 𝒚𝒉𝒂𝒕이 1에 가까워 질수록 𝐂𝐄𝐄는 ∞ \n",
        "  - 0에 가까워질수록 𝐂𝐄𝐄가 작아지며 학습됨\n",
        "\n",
        "\n",
        "<b>𝒚 = 1, 𝐂𝐄𝐄 = -𝓵𝓸𝓰(𝒚𝒉𝒂𝒕)</b>\n",
        "  - 𝒚 = 1, 𝒚𝒉𝒂𝒕이 0에 가까워 질수록 𝐂𝐄𝐄는 ∞\n",
        "  - 1에 가까워질수록 𝐂𝐄𝐄가 작아지며 학습됨\n",
        "\n",
        "[googling  : -log(1-x), -log(x)그래프 비교]\n",
        "\n",
        "<br><br>\n",
        "이러한 기본함수를 가지고 데이터에 최적화 시키기 위해서 <br>\n",
        "y가 0일 때 0으로 잘 예측하도록, 1일때 1으로 잘 예측하도록 <br>\n",
        "w,b값을 바꾸어 모델을 이동시키며 학습한다. <br><br>\n",
        "\n",
        "w가 바뀌면 기울기가 바뀌어, 곡선의 정도에 변화가 생긴다. <br>\n",
        "(S자형태의 곡선은 한 지점에서 3개의 값이 산출되기 때문에 불가능, <br>\n",
        "최대 Step형태의 곡선까지 가능하다.) <br>\n",
        "b값을 바꾸게 되면 b는 모델과 y축이 만나는 지점이기 때문에 <br>\n",
        "결국, 모델이 좌/우로 이동되게 된다.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "w,b 를 구하기 위해선 편미분을 수행하며 이때 오차함수에 CEE를 활용한다. <br>\n",
        "CEE에서는 Entropy 지수를 Error로 활용한다. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbPgODpVzdHI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2-pGrtIfsoz"
      },
      "source": [
        "# I. Cross Entropy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "brALaBNIleeU"
      },
      "source": [
        "\n",
        "* Entropy(불확실성의 척도 ,'불순도') : 불확실성이 낮으면 분류정확도가 높아짐\n",
        "* ```서로 다른 사건의 확률을 곱하여 Entropy를 계산```\n",
        " * y : 실제값, y_hat : 예측값(can be incorrect)\n",
        "* y를 Cross-Entropy의 가중치로 적용\n",
        " * Binary Cross-Entropy Error = –y * log(y_hat) – (1 - y) * log(1 - y_hat)\n",
        " * Categorical Cross-Entropy Error = –y * log(y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lsI64rRxAT"
      },
      "source": [
        "> ## 1) y = 1 vs. y_hat = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNodswhTRjo1",
        "outputId": "9a215b6f-b434-4a9f-93d8-38ef9d9c81e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "y = 1\n",
        "y_hat = 1\n",
        "\n",
        "-y * np.log(y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aoKSQbCS__y"
      },
      "source": [
        "> ## 2) y = 1 vs. y_hat = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh6zMOtoTE7q",
        "outputId": "0327512d-05e3-4ab9-d07d-db37d00a1133"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.210340371976182"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 1\n",
        "y_hat = 0.0001\n",
        "\n",
        "-y * np.log(y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YklxCOKOTQDb"
      },
      "source": [
        "> ## 3) y = 0 vs. y_hat = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJrE5u9GRsnB",
        "outputId": "88f8a5bc-018a-4310-b77c-420cc716d492"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 0\n",
        "y_hat = 0\n",
        "\n",
        "-(1 - y) * np.log(1 - y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CShAr2sbTmNL"
      },
      "source": [
        "> ## 4) y = 0 vs. y_hat = 0.9999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmvhJQ_UT4wn",
        "outputId": "c7bd4184-46d1-4ef3-8060-e37f9b5be49d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.210340371976294"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 0\n",
        "y_hat = 0.9999\n",
        "\n",
        "-(1 - y) * np.log(1 - y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK4Nklzzh_Ed"
      },
      "source": [
        "# II. Information Theory\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IsbYWb1Wi76X"
      },
      "source": [
        "> ## 1) 발생 확률이 서로 다른 사건 A, B, C - Information Gain\n",
        "\n",
        "* Information Gain(정보 이득량)\n",
        "   * 자주 발생하지 않는 사건은 자주 발생하는 사건보다 전달하는 정보량이 많음\n",
        "   * Information Gain(정보 이득량)은 정보의 희귀성(발생가능성)에 반비례\n",
        "   * 정보이득량은  확률 𝐏(𝐗)에 𝓵𝓸𝓰를 취해 줌으로써 구해진다.  \n",
        "   * 확률은 0~ 1사이의 값으므로 확률의 로그값은 항상 음수다. 하지만 정보량은 양수를 가져야함으로 - 를 씌워준다.\n",
        "   * I(x) = -𝓵𝓸𝓰𝐏(𝐗)\n",
        "   * 위의 식은 1 / 𝐏(𝐗) = 𝐏(𝐗)㆒¹ 확률의 역수에 로그를 취한형태인데, 확률의 역수는 적은 양의 정보에 가중치를 부여하는 의미를 갖는다.\n",
        "   <br>\n",
        "\n",
        "* Entropy 지수\n",
        "   * Entropy 지수는 정보 이득량 (확률변수)의 평균 (확률변수의 평균 = 기대값)이다.\n",
        "   * 이산확률분포에서의 평균은 각각 ```확률변수 X값 (정보이득량) 과  확률(발생확률) P값``` 을 곱하여 모두 더하면 구해진다.\n",
        "   * Entropy = 𝐄( -𝓵𝓸𝓰𝐏(𝐗) )\n",
        "   * = sum (-𝓵𝓸𝓰𝐏(𝐗) * 𝐏(𝐗) )\n",
        "   * = -sum( 𝐏(𝐗) * 𝓵𝓸𝓰𝐏(𝐗) )    \n",
        "\n",
        "   <br>\n",
        "   기본적인 엔트로피는  𝐏(𝐗) 𝓵𝓸𝓰𝐏(𝐗)가 같은 사건이어야 하는데 <br>\n",
        "   CEE는 ```q(𝐗)```와 ```𝓵𝓸𝓰𝐏(𝐗)``` 서로 다른 사건이 들어가다보니 크로스 엔트로피라고 한다. <BR>\n",
        "      ```q : 실제분포(모수) ```<br>\n",
        "      ```p : 모델링을 통해 구해진 추정모수 ```<br>\n",
        "   실제값과 예측값이 맞는 경우 0으로 수렴하고, 값이 틀릴경우 값이 커지기 때문에 <br>\n",
        "   실제값과 예측값의 차이를 줄이기 위한 엔트로피로 사용된다. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z4-oHswij-g",
        "outputId": "fa2d92bf-1d6d-42df-ea58-636ed78470cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.105 0.693 2.303\n"
          ]
        }
      ],
      "source": [
        "A = 0.9\n",
        "B = 0.5\n",
        "C = 0.1\n",
        "\n",
        "print('%.3f' % -np.log(A), '%.3f' % -np.log(B), '%.3f' % -np.log(C))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU-3uPyljKQE"
      },
      "source": [
        "> ## 2) AlphaGo와 Apes의 바둑대결 승리 확률 - Degree of Surprise\n",
        "\n",
        "* Degree of Surprise(놀람의 정도)\n",
        " * 예상하기 어려운 정보에 더 높은 가치를 매기는 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBuEPrRDinix",
        "outputId": "b0fd3955-a3d3-49cf-ff45-f7ae1ac223c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.001 6.908\n"
          ]
        }
      ],
      "source": [
        "Alphago = 0.999\n",
        "Apes = 0.001\n",
        "\n",
        "print('%.3f' % -np.log(Alphago), '%.3f' % -np.log(Apes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCnma7QjtpS"
      },
      "source": [
        "# III. Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmYGzfJKkOJL"
      },
      "source": [
        "* 불확실성의 정도\n",
        " * Entropy = E(–log(P(x)))\n",
        "* 확률변수의 평균 정보량(기댓값)\n",
        " * –sum(p(x) * log(p(x)))\n",
        "* 불확실성(Entropy)이 낮으면 분류정확도가 높아짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzSPawrcknfX"
      },
      "source": [
        "> ## 1) 승률이 비슷한 두팀의 Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZerL7OdPk0Zk",
        "outputId": "aaa24fbc-c059-46c5-f74b-ef396cd6160f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "P1 = 0.5\n",
        "P2 = 0.5\n",
        "\n",
        "-P1 * np.log(P1) - P2 * np.log(P2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woYt_azSkuHK"
      },
      "source": [
        "> ## 2) 승률 차이가 큰 두팀의 Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWulX2IEk_18",
        "outputId": "b09b96b2-1601-48aa-dcaa-02b6c9496895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.007907255112232087"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "P1 = 0.999\n",
        "P2 = 0.001\n",
        "\n",
        "-P1 * np.log(P1) - P2 * np.log(P2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4EBM5Pt3R7N"
      },
      "source": [
        "# \n",
        "# \n",
        "# \n",
        "# The End\n",
        "# \n",
        "# \n",
        "# "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c4a217330dafb44a177311bf304eb7602117568170dedb9db52856e3c583233"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
